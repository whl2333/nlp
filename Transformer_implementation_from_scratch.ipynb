{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031559ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy\n",
    "import torch.nn as nn # 学习点1\n",
    "import copy\n",
    "import math\n",
    "import torch.nn.functional as F # 学习点2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30329877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8b67b",
   "metadata": {},
   "source": [
    "# Encoder和Decoder中的sub_layer的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c81b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重点理解\n",
    "#1.计算q,k,v的时候必须要先reshape再permute而不能直接一步reshape到位。以k为例\n",
    "# 直接经过linear projection得到的raw k中不同的head的信息都在最后一维中，如果\n",
    "# 直接reshape成 batch_size,num_heads,seq_len.embedding_dim的大小会导致两个不\n",
    "# 同的head的信息直接被存在了seq_len这一个维度，而没有在head维度被区分开，这显\n",
    "# 然是不能被接受的。因此正确的做法是先将embedding维度拆分为heads*dim_k，然后再\n",
    "# 执行permute操作交换维度信息。\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,num_heads,embedding_dim):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.wq = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.wk = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.wv = nn.Linear(embedding_dim,embedding_dim)\n",
    "        \n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        # query,key,value的尺寸:(batch_size,seq_len,embedding_dim)\n",
    "        batch_size,seq_len_1,embedding_dim = query.shape # 可能是encoder侧，也可能是decoder侧的序列长度\n",
    "        seq_len_2 = key.shape[1] # 一定是encoder侧的序列长度\n",
    "        #q = self.wq(query).reshape(batch_size,seq_len_1,self.num_heads,-1).permute(0,2,1,3)\n",
    "        #k = self.wk(key).reshape(batch_size,seq_len_2,self.num_heads,-1).permute(0,2,1,3)\n",
    "        #v = self.wv(value).reshape(batch_size,seq_len_2,self.num_heads,-1).permute(0,2,1,3)\n",
    "        q = self.wq(query)\n",
    "        k = self.wk(key)\n",
    "        v = self.wv(value)\n",
    "        #print(q.shape,k.shape,v.shape)\n",
    "        # 此时的q,k,v的尺寸:(batch_size,num_heads,seq_len,dim_k)\n",
    "        dim_k = self.embedding_dim//self.num_heads\n",
    "        attent_score = torch.matmul(q,k.transpose(1,2))/math.sqrt(dim_k) # 在cuda上\n",
    "        #print(mask.shape,attent_score.shape)\n",
    "        if mask != None:\n",
    "            mask = mask.to('cuda:0')\n",
    "            attent_score = attent_score.masked_fill(mask,-1e9)\n",
    "        scaled_attent_score = F.softmax(attent_score,-1) # (batch_size,num_heads,seq_len,seq_len)\n",
    "        output = torch.matmul(scaled_attent_score,v).to('cuda:0') # (batch_size,num_heads,seq_len,dim_k)\n",
    "        #print('multi:',output.device)\n",
    "        #output = output_mat.permute(0,2,1,3).reshape(batch_size,seq_len_1,-1) # 重新将size复原为(batch_size,seq_len,embedding_dim)\n",
    "        return output\n",
    "                               \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,embedding_dim,dropout = 0.1):\n",
    "        super(FeedForward,self).__init__()\n",
    "        self.linear_1 = nn.Linear(embedding_dim,4 * embedding_dim)\n",
    "        self.linear_2 = nn.Linear(4 * embedding_dim,embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        return self.linear_2(self.dropout(x))                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b8cb0",
   "metadata": {},
   "source": [
    "# Encoder的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b57444ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,layer,vocab_size,embedding_dim, N=6):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.WordEmbedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.PositionEmbedding  = PositionEncoding()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)]) # Encoder层中的多个layer\n",
    "    def forward(self,x,mask):\n",
    "        x = self.WordEmbedding(x)\n",
    "       # print('Encoder:',x.device)\n",
    "        x = self.PositionEmbedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return x\n",
    "    \n",
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionEncoding,self).__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # size of x: (batch_zize,seq_len,embedding_dim)\n",
    "        # print('PE',x.device)\n",
    "        batch_size,seq_size,embedding_dim = x.shape\n",
    "        pos = torch.arange(seq_size).reshape(-1,1)\n",
    "        i = torch.arange(0,embedding_dim,2).reshape(1,-1)\n",
    "        PE = torch.zeros(seq_size,embedding_dim)\n",
    "        PE[:,0::2]= torch.sin(pos/torch.pow(10000,i/embedding_dim))\n",
    "        PE[:,1::2] = torch.cos(pos/torch.pow(10000,i/embedding_dim))\n",
    "        PE = PE.repeat(batch_size,1).reshape(batch_size,seq_size,embedding_dim).to('cuda:0')\n",
    "        #print('PE:',PE.device)\n",
    "        return x + PE\n",
    "\n",
    "# 单个Encoder Block的实现\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,num_heads,embedding_size):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads,embedding_size) \n",
    "        self.ffn = FeedForward(embedding_dim)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "    \n",
    "    def forward(self,x,enc_mask):\n",
    "        x = self.norm(x + self.multi_head_attention(x,x,x,enc_mask))\n",
    "        x = self.norm(x + self.ffn(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3028ef9e",
   "metadata": {},
   "source": [
    "# Decoder的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db2272e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,layer,vocab_size,embedding_dim,N=6):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.WordEmbedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.PositionEmbedding = PositionEncoding()\n",
    "        self.layers = nn.ModuleList(copy.deepcopy(layer) for _ in range(N))\n",
    "        self.linear = nn.Linear(embedding_dim,vocab_size)\n",
    "        \n",
    "    def forward(self,x,enc_out,dec_mask):\n",
    "        x = self.WordEmbedding(x) # x是idx，输出得到对应的词向量\n",
    "        x = self.PositionEmbedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,enc_out,dec_mask)\n",
    "        return self.linear(x)\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,num_heads,embedding_dim):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(num_heads,embedding_dim)\n",
    "        self.dec_attention = MultiHeadAttention(num_heads,embedding_dim)\n",
    "        self.ffn = FeedForward(embedding_dim)\n",
    "        self.norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    # 这里的dec_seq_mask为什么需要直接传入\n",
    "    def forward(self,x,enc_out,dec_hidden_mask):\n",
    "        dec_seq_len = x.shape[1]\n",
    "        dec_seq_mask = self.get_dec_seq_mask(dec_seq_len)\n",
    "        a = self.masked_multi_head_attention(x,x,x,dec_seq_mask)\n",
    "        #print(a.device)\n",
    "        x = self.norm(self.masked_multi_head_attention(x,x,x,dec_seq_mask) + x)\n",
    "        #print(x.shape,enc_out.shape)\n",
    "        #print(dec_hidden_mask.shape)\n",
    "        x = self.norm(self.dec_attention(x,enc_out,enc_out,dec_hidden_mask) + x)\n",
    "        x = self.norm(self.ffn(x)+x)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dec_seq_mask(dec_seq_len):\n",
    "        ones = torch.ones(1,dec_seq_len,dec_seq_len) # 第一维度是batch维度\n",
    "        valid_mat = torch.tril(ones)\n",
    "        mask_mat = (1 - valid_mat).to(torch.bool)\n",
    "        return mask_mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2fb96f",
   "metadata": {},
   "source": [
    "# Transformer的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f5d9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Transformer,self).__init__()\n",
    "        self.Encoder = encoder\n",
    "        self.Decoder = decoder\n",
    "    def forward(self,x,y,enc_mask,dec_mask):\n",
    "        enc_out = self.Encoder(x,enc_mask)\n",
    "        dec_out = self.Decoder(y,enc_out,dec_mask)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d631ce91",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8cfa1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def normalize_text(content):\n",
    "    content = re.sub(r\"([.!?])\", r\" \\1\", content) # 将标点符号和字符分隔开，到时候方便分离字符和标点符号\n",
    "    content = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", content)\n",
    "    return content\n",
    "\n",
    "def read_raw_data(filename):\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        pairs = [[normalize_text(pair) for pair in line.split('\\t')]for line in lines]\n",
    "        # 过滤\n",
    "        #content = re.sub(r\"([.!?])\", r\" \\1\", content)\n",
    "        #content = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", content)\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def process_raw_data(pairs,reserved_tokens):\n",
    "    # 将输入的raw data转化为\n",
    "    src_lang = [pair[0] for pair in pairs]\n",
    "    tgt_lang = [pair[1] for pair in pairs]\n",
    "    \n",
    "    vocab_src = Vocab(pairs = src_lang,reserved_tokens = reserved_tokens)\n",
    "    vocab_tgt = Vocab(pairs = tgt_lang,reserved_tokens = reserved_tokens)\n",
    "    return vocab_src,vocab_tgt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0547315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self,pairs,reserved_tokens=None,min_freq=2):\n",
    "        self.reserved_tokens = reserved_tokens\n",
    "        self.idx2token = []\n",
    "        self.token2idx = {}\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        token_freq = defaultdict(int) # 记录token出现的频率\n",
    "        for pair in pairs:\n",
    "            for token in pair.split(' '):\n",
    "                token_freq[token] += 1\n",
    "        #print(token_freq)\n",
    "        # 将所有字符装入列表中\n",
    "        unique_tokens = reserved_tokens if reserved_tokens else []\n",
    "        unique_tokens += [token for token, freq in token_freq.items() if freq>=min_freq]\n",
    "        \n",
    "        # 检查是否将UNK的token装入了列表中\n",
    "        if self.UNK_TOKEN not in unique_tokens:\n",
    "            unique_token = [self.UNK_TOKEN] + unique_token\n",
    "        \n",
    "        # 构造idx2token和token2idx\n",
    "        for token in unique_tokens:\n",
    "            self.idx2token.append(token)\n",
    "            self.token2idx[token]=len(self.idx2token)-1\n",
    "        \n",
    "        self.unk = self.token2idx[self.UNK_TOKEN] # 记录unk在词表中的下标，用于标记语料中未出现的token\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 返回词表长度\n",
    "        return len(self.idx2token)\n",
    "    \n",
    "    def __getitem__(self,token):\n",
    "        # 输入token，返回idx\n",
    "        return self.token2idx.get(token,self.unk)\n",
    "    \n",
    "    def convert_token_to_idx(self,tokens):\n",
    "        # 将输入的一串token转化为idx并以列表形式输出\n",
    "        return [self[token] for token in tokens]\n",
    "    \n",
    "    def convert_idx_to_token(self,ids):\n",
    "        # 将输入的一串ids转化为token并以列表形式输出\n",
    "        return [self.idx2token[idx] for idx in ids]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "164f693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self,vocab_src,vocab_tgt,pairs):\n",
    "        self.src_vocab,self.tgt_vocab = vocab_src, vocab_tgt\n",
    "        self.src_lang = [pair[0] for pair in pairs]\n",
    "        self.tgt_lang = [pair[1] for pair in pairs]\n",
    "        \n",
    "    #__len\n",
    "    def __len__(self):\n",
    "        return len(self.src_lang)\n",
    "    # 实现索引数据集中的某一个元素\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        src_tokens = self.src_lang[idx]\n",
    "        tgt_tokens = self.tgt_lang[idx]\n",
    "        encoder_input = vocab_src.convert_token_to_idx(src_tokens.split())\n",
    "        decoder_input = vocab_tgt.convert_token_to_idx(['<BOS>'] + tgt_tokens.split())\n",
    "        decoder_output = vocab_tgt.convert_token_to_idx(tgt_tokens.split() + ['EOS'])\n",
    "        return encoder_input,decoder_input,decoder_output\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    encoder_raw_input = [torch.tensor(example[0]) for example in batch]\n",
    "    decoder_raw_input = [torch.tensor(example[1]) for example in batch]\n",
    "    decoder_raw_output  = [torch.tensor(example[2]) for example in batch]\n",
    "    \n",
    "    encoder_input = pad_sequence(encoder_raw_input,batch_first=True,padding_value=vocab_src['[<PAD>]'])\n",
    "    decoder_input = pad_sequence(decoder_raw_input,batch_first=True,padding_value=vocab_tgt['[<PAD>]'])\n",
    "    decoder_output = pad_sequence(decoder_raw_output,batch_first=True,padding_value=vocab_tgt['[<PAD>]'])\n",
    "    return encoder_input,decoder_input,decoder_output\n",
    "\n",
    "def mask_process(enc_input,dec_input):\n",
    "\n",
    "    # 生成encoder中的mask\n",
    "    enc_valid_pos = torch.where(enc_input>0.0,1.0,0.0)\n",
    "    enc_valid_pos = torch.unsqueeze(enc_valid_pos,2)\n",
    "    enc_valid_mat = torch.bmm(enc_valid_pos,enc_valid_pos.transpose(1,2))\n",
    "    enc_mask = (1 - enc_valid_mat).to(torch.bool)\n",
    "    \n",
    "    # 生成encoder_decoder_mask\n",
    "    dec_valid_pos = torch.where(dec_input > 0.0,1.0,0.0)\n",
    "    dec_valid_pos = torch.unsqueeze(dec_valid_pos,2)\n",
    "    enc_dec_mat = torch.bmm(dec_valid_pos,enc_valid_pos.transpose(1,2)) #前后顺序不能调换，否则会导致维度信息错误\n",
    "    enc_dec_mask = (1 - enc_dec_mat).to(torch.bool)\n",
    "    \n",
    "    return enc_mask,enc_dec_mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9e44140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0finish training\n",
      "epoch: 1finish training\n",
      "epoch: 2finish training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 266240 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m         avg_loss\u001b[38;5;241m.\u001b[39mappend(total_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(x))\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss\n\u001b[1;32m---> 64\u001b[0m loss \u001b[38;5;241m=\u001b[39m train(model,loss_func,dataloader,epochs)\n\u001b[0;32m     65\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(loss)\n\u001b[0;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[76], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loss_func, dataloader, epochs)\u001b[0m\n\u001b[0;32m     44\u001b[0m enc_mask,enc_dec_mask \u001b[38;5;241m=\u001b[39m mask_process(x,y_in)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#print(enc_mask.device,enc_dec_mask.device)\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(x,y_in,enc_mask,enc_dec_mask)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# print(y_pred.device)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# print(y_pred.shape)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[72], line 7\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y, enc_mask, dec_mask)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,y,enc_mask,dec_mask):\n\u001b[1;32m----> 7\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncoder(x,enc_mask)\n\u001b[0;32m      8\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDecoder(y,enc_out,dec_mask)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[70], line 10\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m  x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWordEmbedding(x)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# print('Encoder:',x.device)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m  x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPositionEmbedding(x)\n\u001b[0;32m     11\u001b[0m  \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     12\u001b[0m      x \u001b[38;5;241m=\u001b[39m layer(x,mask)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[70], line 28\u001b[0m, in \u001b[0;36mPositionEncoding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m PE[:,\u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(pos\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m10000\u001b[39m,i\u001b[38;5;241m/\u001b[39membedding_dim))\n\u001b[0;32m     27\u001b[0m PE[:,\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(pos\u001b[38;5;241m/\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m10000\u001b[39m,i\u001b[38;5;241m/\u001b[39membedding_dim))\n\u001b[1;32m---> 28\u001b[0m PE \u001b[38;5;241m=\u001b[39m PE\u001b[38;5;241m.\u001b[39mrepeat(batch_size,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size,seq_size,embedding_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#print('PE:',PE.device)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m PE\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 266240 bytes."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = r'C:\\Users\\吴昊伦\\Desktop\\nlp\\data\\data\\eng-fra.txt'\n",
    "reserved_tokens = ['<UNK>','<BOS>','<EOS>','<PAD>']\n",
    "embedding_dim = 512\n",
    "num_heads =  8\n",
    "epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "pairs = read_raw_data(filename)\n",
    "vocab_src,vocab_tgt = process_raw_data(pairs,reserved_tokens)\n",
    "\n",
    "dataset = TranslationDataset(vocab_src,vocab_tgt,pairs)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size,collate_fn=collate_fn,shuffle=True)\n",
    "\n",
    "#print(next(iter(dataloader)))\n",
    "src_size = len(vocab_src) # 源文本的词表长度\n",
    "tgt_size = len(vocab_tgt) # 目标文本的词表长度\n",
    "#print(src_size,tgt_size)\n",
    "\n",
    "# 实例化encoder decoder和transformer\n",
    "encoder = Encoder(EncoderLayer(num_heads,embedding_dim),src_size,embedding_dim)\n",
    "decoder = Decoder(DecoderLayer(num_heads,embedding_dim),tgt_size,embedding_dim)\n",
    "model = Transformer(encoder,decoder).to('cuda:0')\n",
    "#print(next(model.parameters()).device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=vocab_tgt['<PAD>'])\n",
    "\n",
    "def train(model,loss_func,dataloader,epochs):\n",
    "    \n",
    "    # batch normalization用的是每一个batch的均值和方差，启用dropout\n",
    "    model.train()\n",
    "    \n",
    "    avg_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x,y_in,y_out in dataloader:\n",
    "            #print(x)\n",
    "            #print(y_in)\n",
    "            # 将向量载入gpu中\n",
    "            x = x.to('cuda:0')\n",
    "            y_in = y_in.to('cuda:0')\n",
    "            y_out = y_out.to('cuda:0')\n",
    "            enc_mask,enc_dec_mask = mask_process(x,y_in)\n",
    "            #print(enc_mask.device,enc_dec_mask.device)\n",
    "            y_pred = model(x,y_in,enc_mask,enc_dec_mask)\n",
    "            # print(y_pred.device)\n",
    "            # print(y_pred.shape)\n",
    "            \n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = loss_func(y_pred.permute(0,2,1),y_out) #permute的作用是什么\n",
    "            total_loss += loss\n",
    "            \n",
    "            # 更新梯度\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('epoch: '+ str(epoch)+ 'finish training')\n",
    "        avg_loss.append(total_loss/len(x))\n",
    "    return avg_loss\n",
    "\n",
    "loss = train(model,loss_func,dataloader,epochs)\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b627b70",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9b9fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 5, 0, 0],\n",
      "        [1, 1, 4, 6]]) \n",
      " tensor([[5, 2, 5, 3],\n",
      "        [2, 7, 7, 0]])\n"
     ]
    }
   ],
   "source": [
    "max_num_src_words = 8 # 源单词序列的最大长度，即总共有多少个单词\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "batch_size = 2 # 总共有几个序列\n",
    "\n",
    "embedding_size = 8 # 原论文使用了512\n",
    "\n",
    "src_len =  torch.tensor([2,4]).to(torch.int32)\n",
    "tgt_len = torch.tensor([4,3]).to(torch.int32)\n",
    "\n",
    "# 单词索引构成的源句子和目标句子，注意0索引表示padding，单词索引从1开始\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_src_words,(L,)),(0,max(src_len) - L)),0) \n",
    "                     for L in src_len]) # 源序列输入时要保证长度一致，需要pad来填充,然后先将序列的tensor扩展成2维的再拼接起来\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1,max_num_tgt_words,(L,)),(0,max(tgt_len) - L)),0)\n",
    "                     for L in tgt_len])\n",
    "print(src_seq,'\\n',tgt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f257d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造embedding,可以用pytorch的api实现,会取出对应索引对应的embedding\n",
    "src_embedding_table = nn.Embedding(max_num_src_words + 1, embedding_size) # 需要+1是因为padding占据了一位\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, embedding_size)\n",
    "\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac3bdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 2.4201, -1.3914,  0.2174,  0.6065, -0.7812,  0.3648,  0.1808,  1.5536],\n",
       "        [ 1.3456, -0.4914, -0.7922, -0.0246,  0.7840, -0.4474, -0.7568,  0.6310],\n",
       "        [-0.7511, -1.8326,  0.5105, -0.6110,  0.9412,  0.8918, -0.5526, -0.2191],\n",
       "        [-0.8040, -1.2852, -1.3060, -0.2187, -1.4082, -0.9575,  0.7691, -0.0717],\n",
       "        [-0.4035,  0.2432,  0.9620, -0.3032,  0.5702, -0.8582,  0.5748, -1.5265],\n",
       "        [-1.1838, -1.8929,  0.0855,  0.4732, -0.2798,  0.6351, -0.6859, -0.1743],\n",
       "        [ 0.8086,  0.2111,  1.0780,  1.9412, -0.8829, -0.8943, -0.3597, -0.6305],\n",
       "        [ 0.3129, -1.0148, -1.0447,  0.2352, -0.4093,  0.0517,  0.2710,  1.3452],\n",
       "        [ 0.8691, -0.6900, -0.5903, -0.8140, -2.2298,  1.3410, -1.4226,  1.0431]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_embedding_table.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc38b6e",
   "metadata": {},
   "source": [
    "# Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08b80eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_mat = torch.arange(max(src_len)).reshape(-1,1)\n",
    "i_mat = torch.pow(10000,torch.arange(0,embedding_size,2).reshape(1,-1)/embedding_size)\n",
    "PE_table = torch.zeros(max(src_len),embedding_size)\n",
    "PE_table[:,0::2] = torch.sin(pos_mat/i_mat)\n",
    "PE_table[:,1::2] = torch.cos(pos_mat/i_mat)\n",
    "\n",
    "PE = nn.Embedding(max(src_len),embedding_size)\n",
    "PE.weight = nn.Parameter(PE_table,requires_grad = False)\n",
    "\n",
    "# 获取位置信息，得到源序列和目标序列中每一个序列对应的位置编码\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)),0) for _ in src_len])\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)),0) for _ in tgt_len])\n",
    "src_pe = PE(src_pos)\n",
    "tgt_pe = PE(tgt_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1e9c2",
   "metadata": {},
   "source": [
    "# Encoder中的Self-Attention Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79ccfffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [ True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_encoder_pos_mat = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L),(0,max(src_len) - L)),0) for L in src_len]),1)\n",
    "valid_encoder_mat = torch.bmm(valid_encoder_pos_mat.transpose(1,2),valid_encoder_pos_mat)\n",
    "mask_encoder_mat = (1 - valid_encoder_mat).to(torch.bool)\n",
    "mask_encoer_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56a28c",
   "metadata": {},
   "source": [
    "#    intra-attention的Mask？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0532da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cdb6fb2",
   "metadata": {},
   "source": [
    "# 构造decoder的self-attention的mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4761673",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_decoder_tri_matrix = torch.cat([torch.unsqueeze(F.pad(torch.tril(torch.ones(L,L)),(0,max(tgt_len)-L,0,max(tgt_len)-L)),0) \n",
    "                                                      for L in tgt_len]) # pad的方向是左右上下\n",
    "mask_decoder_tri_matrix = (1 - valid_decoder_tri_matrix).to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fdfe702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [False, False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  True],\n",
       "         [False, False,  True,  True],\n",
       "         [False, False, False,  True],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_decoder_tri_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4629e48",
   "metadata": {},
   "source": [
    "# 构造self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q,K,V,masked_atten):\n",
    "    # shape of Q,K,V: (batch_size*num_heads*seq_len*)\n",
    "    score = torch.bmm(Q,K.transpose(-2,-1))/torch.sqrt(embedding_size)\n",
    "    masked_score = score.masked_fill(masked_atten,-1e9)\n",
    "    prob = F.softmax(masked_score,-1)\n",
    "    context = torch.bmm(prob,V)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1fcc762",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 2\n",
    "embedding_dim = 8\n",
    "#pos = torch.arange(seq_len).reshape(-1,1).repeat(1,batch_size)\n",
    "pos = torch.arange(seq_len).reshape(-1,1).repeat(batch_size,1)\n",
    "#i_dim = torch.arange(0,embedding_dim,2).reshape(1,-1).repeat(batch_size,1)\n",
    "i_dim = torch.arange(0,embedding_dim,2).reshape(1,-1).repeat(1,batch_size)\n",
    "aa = torch.sin(pos/torch.pow(10000,i_dim/embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a3089eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [0],\n",
       "         [1],\n",
       "         [2],\n",
       "         [3],\n",
       "         [4]]),\n",
       " tensor([[0, 2, 4, 6, 0, 2, 4, 6]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos,i_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c89d7ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0998,  0.0100,  0.0010,  0.8415,  0.0998,  0.0100,  0.0010],\n",
       "        [ 0.9093,  0.1987,  0.0200,  0.0020,  0.9093,  0.1987,  0.0200,  0.0020],\n",
       "        [ 0.1411,  0.2955,  0.0300,  0.0030,  0.1411,  0.2955,  0.0300,  0.0030],\n",
       "        [-0.7568,  0.3894,  0.0400,  0.0040, -0.7568,  0.3894,  0.0400,  0.0040],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0998,  0.0100,  0.0010,  0.8415,  0.0998,  0.0100,  0.0010],\n",
       "        [ 0.9093,  0.1987,  0.0200,  0.0020,  0.9093,  0.1987,  0.0200,  0.0020],\n",
       "        [ 0.1411,  0.2955,  0.0300,  0.0030,  0.1411,  0.2955,  0.0300,  0.0030],\n",
       "        [-0.7568,  0.3894,  0.0400,  0.0040, -0.7568,  0.3894,  0.0400,  0.0040]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.reshape(batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "386aeaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "batch_size = 2\n",
    "embedding_dim = 8\n",
    "#pos = torch.arange(seq_len).reshape(-1,1).repeat(1,batch_size)\n",
    "pos = torch.arange(seq_len).reshape(-1,1)\n",
    "#i_dim = torch.arange(0,embedding_dim,2).reshape(1,-1).repeat(batch_size,1)\n",
    "i_dim = torch.arange(0,embedding_dim,2).reshape(1,-1)\n",
    "PE = torch.zeros(seq_len,embedding_dim)\n",
    "PE[:,0::2] = torch.sin(pos/torch.pow(10000,i_dim/embedding_dim))\n",
    "PE[:,1::2] = torch.cos(pos/torch.pow(10000,i_dim/embedding_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "392f4f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "          9.9920e-01,  4.0000e-03,  9.9999e-01]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7d02a55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "           9.9920e-01,  4.0000e-03,  9.9999e-01]],\n",
       "\n",
       "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "           9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "         [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "           9.9920e-01,  4.0000e-03,  9.9999e-01]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE.repeat(batch_size,1).reshape(batch_size,seq_len,embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db84f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "embedding_dim = 8\n",
    "num_heads = 2\n",
    "\n",
    "a = torch.randn(batch_size,seq_len,embedding_dim)\n",
    "b = torch.randn(batch_size,seq_len,embedding_dim)\n",
    "c = torch.randn(batch_size,seq_len,embedding_dim)\n",
    "wq = nn.Linear(embedding_dim,embedding_dim)\n",
    "wk = nn.Linear(embedding_dim,embedding_dim)\n",
    "wv = nn.Linear(embedding_dim,embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "aaae6d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2291, -0.1363,  0.4670, -0.1756],\n",
      "          [ 0.1886, -0.1864,  0.4660, -0.1887],\n",
      "          [ 0.2121, -0.1470,  0.4563, -0.1611],\n",
      "          [ 0.1453, -0.1846,  0.4733, -0.2189],\n",
      "          [ 0.2455, -0.2170,  0.5185, -0.2419],\n",
      "          [ 0.3388, -0.1870,  0.5067, -0.1635]],\n",
      "\n",
      "         [[-0.1176,  0.6423,  0.2355,  0.1742],\n",
      "          [ 0.0247,  0.4520,  0.3118,  0.0976],\n",
      "          [ 0.1012,  0.3430,  0.3379,  0.0537],\n",
      "          [ 0.0823,  0.3480,  0.3429,  0.0338],\n",
      "          [-0.0160,  0.4432,  0.3425,  0.0157],\n",
      "          [-0.0215,  0.4969,  0.2840,  0.1019]]],\n",
      "\n",
      "\n",
      "        [[[-0.0488, -0.2397,  0.1415, -0.6647],\n",
      "          [-0.0662, -0.2207,  0.1350, -0.7083],\n",
      "          [-0.0219, -0.2494,  0.1512, -0.6331],\n",
      "          [-0.0363, -0.2556,  0.1776, -0.6930],\n",
      "          [-0.0749, -0.2134,  0.1155, -0.6936],\n",
      "          [-0.1219, -0.1628,  0.0692, -0.7518]],\n",
      "\n",
      "         [[-0.5035, -0.3416,  0.0799, -0.3281],\n",
      "          [-0.5935, -0.2866, -0.1025, -0.3248],\n",
      "          [-0.5911, -0.1463, -0.1422, -0.2937],\n",
      "          [-0.6336, -0.1746, -0.2204, -0.3011],\n",
      "          [-0.6151, -0.1772, -0.2027, -0.2974],\n",
      "          [-0.6218, -0.2856, -0.1727, -0.3248]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) torch.Size([2, 2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "key = a\n",
    "query = b\n",
    "value = c\n",
    "\n",
    "\n",
    "k = wk(key) #(batch_size,seq_len_embedding_dim)\n",
    "# print('raw k:',k,k.shape)\n",
    "k = k.reshape(batch_size,seq_len,num_heads,-1) # batch_size,seq_len,num_heads,dim_k\n",
    "# print('reshape k:',k,k.shape)\n",
    "k=k.permute(0,2,1,3) # batch_size,num_heads.seq_len,dim_k\n",
    "# print('final k:',k,k.shape)\n",
    "\n",
    "q = wq(query).reshape(batch_size,seq_len,num_heads,-1).permute(0,2,1,3)\n",
    "v = wv(query).reshape(batch_size,seq_len,num_heads,-1).permute(0,2,1,3) \n",
    "dim_k = embedding_dim//num_heads\n",
    "\n",
    "attent_score = torch.matmul(q,k.transpose(2,3))/math.sqrt(dim_k)\n",
    "#print(attent_score,attent_score.shape) \n",
    "scaled_attent_score = F.softmax(attent_score,-1) # (batch_size,num_heads,seq_len,seq_len)\n",
    "#print(scaled_attent_score,scaled_attent_score.shape) \n",
    "context_mat = torch.matmul(scaled_attent_score,v) # (batch_size,)\n",
    "print(context_mat,context_mat.shape) #(batch_size,num_heads,seq_len,dim_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5b24369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.7317, -0.0352,  0.1948,  0.4176],\n",
       "          [-0.1013, -0.7229,  0.3077, -0.7610],\n",
       "          [-0.1686, -0.1843,  0.7883,  0.1585],\n",
       "          [ 0.2514, -0.1433, -0.3918, -0.6952],\n",
       "          [ 0.7596,  0.7998,  0.5008,  0.0423],\n",
       "          [-0.0270,  0.3743, -0.1011, -0.2615]],\n",
       "\n",
       "         [[-0.6680, -0.8831, -0.6066,  0.8854],\n",
       "          [ 0.2504, -0.0797, -1.4161, -0.8618],\n",
       "          [ 0.1207, -0.3178,  0.1992,  0.5513],\n",
       "          [-0.4767, -0.6457, -0.4611, -0.5847],\n",
       "          [ 0.2248,  0.0268, -0.3078,  0.8197],\n",
       "          [ 0.2010, -0.3610,  0.4303, -0.2490]]],\n",
       "\n",
       "\n",
       "        [[[-0.1199, -0.4672,  1.0029,  0.7095],\n",
       "          [ 0.2995,  0.8645, -0.9686, -0.5742],\n",
       "          [ 1.1665,  0.2206,  0.1515, -0.1391],\n",
       "          [ 0.0688,  0.2192,  0.3753, -0.5280],\n",
       "          [ 0.1752,  0.1438, -0.3738,  1.6293],\n",
       "          [-0.4363, -0.1756,  0.1196, -0.1044]],\n",
       "\n",
       "         [[ 0.3226,  0.3679, -0.3191,  0.9891],\n",
       "          [-0.4345, -0.6546,  0.8613, -0.1316],\n",
       "          [ 1.5812,  0.5335,  0.4464, -1.2443],\n",
       "          [ 0.0748, -0.4716,  0.7139, -0.3644],\n",
       "          [ 0.9454,  0.0101,  0.0243,  0.0879],\n",
       "          [-0.0703,  0.6432, -0.0288, -0.2477]]]],\n",
       "       grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = wq(key) #(batch_size,seq_len_embedding_dim)\n",
    "k = k.reshape(batch_size,num_heads,seq_len,-1) # batch_size,seq_lne,num_heads,dim_k\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72acea65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4592]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e09ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
